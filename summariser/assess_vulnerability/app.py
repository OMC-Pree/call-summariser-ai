"""
Vulnerability Assessment Lambda - Step Functions workflow step
Performs detailed FCA FG21/1 vulnerability assessment when case check identifies vulnerabilities
"""
import json
import os
from typing import Optional, Dict, Any
from datetime import datetime, timezone, timedelta
from decimal import Decimal

from utils import helper
from utils.error_handler import lambda_error_handler
from utils.prompt_management import invoke_with_prompt_management, get_prompt_arn_from_parameter_store
from utils.aws_clients import AWSClients
from constants import *

# Use centralized AWS clients
bedrock = AWSClients.bedrock_runtime()
s3 = AWSClients.s3()
ssm = AWSClients.ssm()

# DynamoDB client for assessment-results table
import boto3
dynamodb = boto3.resource('dynamodb')
ASSESSMENT_RESULTS_TABLE = os.environ.get('ASSESSMENT_RESULTS_TABLE', 'assessment-results')
assessment_table = dynamodb.Table(ASSESSMENT_RESULTS_TABLE)

# Prompt Management configuration
USE_PROMPT_MANAGEMENT = os.environ.get("USE_PROMPT_MANAGEMENT", "true").lower() == "true"
PROMPT_PARAM_NAME = os.environ.get("PROMPT_PARAM_NAME_VULNERABILITY", "/call-summariser/prompts/vulnerability-assessment/current")

# Cache for prompt ARN (loaded once per Lambda container)
_prompt_arn_cache = {}

# Model configuration
VULNERABILITY_MODEL_ID = "anthropic.claude-3-7-sonnet-20250219-v1:0"
VULNERABILITY_MODEL_VERSION = "claude-3.7-sonnet-20250219"
VULNERABILITY_PROMPT_VERSION = "1.0"


def get_prompt_arn() -> Optional[str]:
    """Get prompt ARN from Parameter Store (cached for Lambda container lifetime)"""
    return get_prompt_arn_from_parameter_store(
        param_name=PROMPT_PARAM_NAME,
        cache_dict=_prompt_arn_cache,
        use_prompt_management=USE_PROMPT_MANAGEMENT
    )


def get_transcript_from_s3(s3_key: str) -> str:
    """Fetch redacted transcript from S3"""
    response = s3.get_object(Bucket=SUMMARY_BUCKET, Key=s3_key)
    return response['Body'].read().decode('utf-8')


def get_vulnerability_tool():
    """
    Create a tool definition for structured vulnerability assessment output.
    This ensures the LLM returns valid JSON matching our schema.
    """
    return {
        "toolSpec": {
            "name": "submit_vulnerability_assessment",
            "description": "Submit the customer vulnerability assessment. Provide rating, vulnerability types with evidence, and detailed reasoning.",
            "inputSchema": {
                "json": {
                    "type": "object",
                    "properties": {
                        "rating": {
                            "type": "string",
                            "description": "Vulnerability rating: Critical/5, High/4, Medium/3, Low/2, Marginal/1, None/0",
                            "enum": ["Critical/5", "High/4", "Medium/3", "Low/2", "Marginal/1", "None/0"]
                        },
                        "vulnerability_types": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of FCA FG21/1 vulnerability categories identified (e.g., 'Health: Chronic Illness')"
                        },
                        "evidence": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Direct quotes from transcript supporting each vulnerability type (one quote per type)"
                        },
                        "reasoning": {
                            "type": "string",
                            "description": "Detailed explanation of why this rating was assigned, referencing specific evidence and explaining how vulnerabilities combine to justify the severity rating"
                        }
                    },
                    "required": ["rating", "vulnerability_types", "evidence", "reasoning"]
                }
            }
        }
    }


def _save_vulnerability_assessment(meeting_id: str, assessment_data: dict, transcript_key: str):
    """
    Save vulnerability assessment to assessment-results DynamoDB table.
    """
    try:
        expires_at = int((datetime.now(timezone.utc) + timedelta(days=90)).timestamp())

        assessment_table.put_item(Item={
            'meeting_id': meeting_id,
            'assessment_id': f"vulnerability#{meeting_id}",
            'assessment_type': 'vulnerability',

            # Assessment results
            'vulnerability_rating': assessment_data.get('rating'),
            'vulnerability_types': assessment_data.get('vulnerability_types', []),
            'evidence_quotes': assessment_data.get('evidence', []),
            'reasoning': assessment_data.get('reasoning', ''),

            # Metadata
            'ai_version': VULNERABILITY_PROMPT_VERSION,
            'model_name': VULNERABILITY_MODEL_VERSION,
            'review_status': 'pending',
            'created_at': datetime.now(timezone.utc).isoformat(),
            'assessed_at': datetime.now(timezone.utc).isoformat(),

            # Links
            'transcript_s3_key': transcript_key,

            # TTL
            'expires_at': expires_at
        })

        helper.log_json("INFO", "VULNERABILITY_ASSESSMENT_SAVED",
                       meetingId=meeting_id,
                       rating=assessment_data.get('rating'),
                       vulnerabilityCount=len(assessment_data.get('vulnerability_types', [])))

    except Exception as e:
        # Log error but don't fail the Lambda
        helper.log_json("ERROR", "ASSESSMENT_TABLE_UPDATE_FAILED",
                       meetingId=meeting_id,
                       error=str(e))


@lambda_error_handler()
def lambda_handler(event, context):
    """
    Perform detailed vulnerability assessment on customer.

    Input:
        - redactedTranscriptKey: str (S3 key to redacted transcript)
        - meetingId: str

    Output:
        - vulnerabilityData: dict
        - vulnerabilityRating: str
    """
    transcript_key = event.get("redactedTranscriptKey")
    meeting_id = event.get("meetingId")

    if not transcript_key:
        raise ValueError("redactedTranscriptKey is required")

    if not meeting_id:
        raise ValueError("meetingId is required")

    helper.log_json("INFO", "VULNERABILITY_ASSESSMENT_START",
                   meetingId=meeting_id,
                   transcriptKey=transcript_key)

    # Fetch transcript from S3
    full_transcript = get_transcript_from_s3(transcript_key)

    helper.log_json("INFO", "VULNERABILITY_ASSESSMENT_TRANSCRIPT_LOADED",
                   meetingId=meeting_id,
                   transcript_length=len(full_transcript),
                   transcript_tokens_approx=len(full_transcript) // 4)

    # Get vulnerability assessment using Prompt Management
    vulnerability_tool = get_vulnerability_tool()
    prompt_arn = get_prompt_arn()

    if prompt_arn:
        variables = {
            "cleaned_transcript": full_transcript
        }

        resp, latency_ms = invoke_with_prompt_management(
            prompt_arn=prompt_arn,
            variables=variables,
            model_id=VULNERABILITY_MODEL_ID,
            tools=[vulnerability_tool],
            tool_choice={"tool": {"name": "submit_vulnerability_assessment"}},
            max_tokens_override=3000
        )
    else:
        helper.log_json("ERROR", "NO_PROMPT_ARN", message="Prompt Management disabled or failed")
        raise ValueError("Prompt Management is required but prompt ARN not available")

    # Extract structured output from tool use
    output_message = resp.get("output", {}).get("message", {})
    content_blocks = output_message.get("content", [])

    tool_use_block = None
    for block in content_blocks:
        if "toolUse" in block:
            tool_use_block = block["toolUse"]
            break

    if not tool_use_block:
        raise ValueError("No tool use block found in response")

    stop_reason = resp.get("stopReason", "")
    if stop_reason == "max_tokens":
        helper.log_json("ERROR", "VULNERABILITY_ASSESSMENT_TRUNCATED",
                       meetingId=meeting_id,
                       message="Response hit max_tokens - increase max_tokens_override")
        raise ValueError(f"Response truncated at max_tokens for meeting {meeting_id}")

    validated_json = tool_use_block["input"]

    # Calculate usage and cost
    usage = resp.get("usage", {})
    cost_breakdown = helper.calculate_bedrock_cost(usage, model_id="claude-3-7-sonnet")

    log_data = {
        "meetingId": meeting_id,
        "operation": "vulnerability_assessment",
        "latency_ms": latency_ms,
        "stop_reason": stop_reason,
        "input_tokens": usage.get("inputTokens", 0),
        "output_tokens": usage.get("outputTokens", 0),
        "structured_output": True,
        "cost_usd": cost_breakdown["total_cost"],
        "input_cost_usd": cost_breakdown["input_cost"],
        "output_cost_usd": cost_breakdown["output_cost"]
    }

    if "cacheReadInputTokens" in usage:
        log_data["cache_read_tokens"] = usage.get("cacheReadInputTokens", 0)
        log_data["cache_creation_tokens"] = usage.get("cacheCreationInputTokens", 0)
        log_data["cache_read_cost_usd"] = cost_breakdown.get("cache_read_cost", 0)
        log_data["cache_write_cost_usd"] = cost_breakdown.get("cache_write_cost", 0)
        log_data["cache_savings_usd"] = cost_breakdown.get("cache_savings", 0)

    helper.log_json("INFO", "VULNERABILITY_ASSESSMENT_LLM_OK", **log_data)

    # Save to DynamoDB
    _save_vulnerability_assessment(meeting_id, validated_json, transcript_key)

    return {
        "vulnerabilityData": validated_json,
        "vulnerabilityRating": validated_json.get("rating", "Unknown")
    }
